<?xml version="1.0" encoding="UTF-8"?>
<project basedir="." name="download-convert-upload" default="build">
	<property file="upload.properties"/>
	<property name="baseurl" value="http://www.data.gouv.fr/fr/"/>
		
	<!-- Necessary to set full path in Linux as PATH in not available in cron job -->
	<condition property="curl" value="curl.exe" else="/usr/bin/curl">
		<os family="windows"/>
	</condition>
	<condition property="tarql" value="tarql.bat" else="/home/admin/tarql/bin/tarql">
		<os family="windows"/>
	</condition>
	<condition property="fuseki" value="fuseki-server.bat" else="/home/admin/fuseki/fuseki-server">
		<os family="windows"/>
	</condition>
	<condition property="livedataDownloaded" value="true" else="false">
		<available file="rdf/livedata.trig"></available>
	</condition>
	<tstamp>
		<!-- With - to be filename-compatible -->
		<format property="now"
			pattern="yyyy-MM-dd'T'HH-mm-ss"/>
	</tstamp>
	<tstamp>
		<!-- Standard dateTime format -->
		<format property="start"
			pattern="yyyy-MM-dd'T'HH:mm:ss"/>
	</tstamp>
	
	<extension-point name="quick" depends="publish"/>
	<extension-point name="build" depends="convert, publish"/>
	<extension-point name="test" depends="convert"/>
	
	<target name="download">
  	<echo>[DOWNLOAD AND EXTRACT]</echo>
		<echo>Downloading ZIP...</echo>
  	<get
  		verbose="false"
  		src="http://www.data.gov.uk/data/dumps/data.gov.uk-ckan-meta-data-latest.csv.zip"
  		dest="csv/dump.zip"/>
		<echo>Extracting CSV dumps...</echo>
		<unzip src="csv/dump.zip" dest="csv/"/>
  </target>
	
	<target name="clean" depends="download">
		<echo>[CLEAN]</echo>
		<mkdir dir="csv/temp/"/>
		<echo>Removing empty lines that mess with TARQL querying...</echo>
		<copy todir="csv/temp/">
			<fileset dir="csv/" includes="*.csv"/>
			<filterchain>
				<ignoreblank/>	
			</filterchain>
		</copy>
		<move todir="csv/">
			<fileset dir="csv/temp/" includes="*.csv"/>
		</move>
		<echo>Replacing " " with "_" in headers to ease TARQL processing...</echo>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Top level organisation" replace=",Top_level_organisation"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Import source" replace=",Import_source"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Geographic Coverage" replace=",Geographic_Coverage"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",License Id" replace=",License_Id"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Metadata Created" replace=",Metadata_Created"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Metadata Modified" replace=",Metadata_Modified"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",ODI Certificate URL" replace=",ODI_Certificate_URL"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Temporal Coverage From" replace=",Temporal_Coverage_From"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Temporal Coverage To" replace=",Temporal_Coverage_To"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Primary Theme" replace=",Primary_Theme"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Secondary Themes" replace=",Secondary_themes"/>
		<replaceregexp flags="gs" file="csv/datasets.csv" encoding="utf-8" match=",Update Frequency," replace=",Update_Frequency,"/>		
		
		<replaceregexp flags="gs" file="csv/resources.csv" encoding="utf-8" match="Dataset Name," replace="Dataset_Name,"/>
		<replaceregexp flags="gs" file="csv/resources.csv" encoding="utf-8" match=",Resource ID" replace=",Resource_ID"/>
		<replaceregexp flags="gs" file="csv/resources.csv" encoding="utf-8" match=",Top level organization" replace=",Top_level_organization"/>
		
		<echo>Removing ""[""" and """]"" from datasets.csv...</echo>
		<replace file="csv/datasets.csv" encoding="utf-8" token='"[""' />
		<replace file="csv/datasets.csv" encoding="utf-8" token='""]"' />	
		
		<echo>Deleting temp directory...</echo>
		<delete dir="csv/temp/"/>
	</target>
	
	<target name="convert" depends="clean">
		<echo>[CONVERT]</echo>
		<echo>Querying datasets.csv with a SPARQL CONSTRUCT using TARQL (thanks @cygri)...</echo>
		<exec executable="${tarql}" searchpath="true" output="rdf/datasets.ttl">
			<arg line="sparql/construct-datasets.rq"/>
		</exec>
		<echo>Querying resources.csv with a SPARQL CONSTRUCT using TARQL (thanks @cygri)...</echo>
		<exec executable="${tarql}" searchpath="true" output="rdf/resources.ttl">
			<arg line="sparql/construct-resources.rq"/>
		</exec>
		<!--<antcall target="upload_temp"/>-->
	</target>
	
	<target name="upload_temp">
		<echo>[TEMPORARY UPLOAD]</echo>
		<echo>Uploading datasets.ttl to the postprocessing repository...</echo>
		<exec executable="${curl}" searchpath="true" failonerror="true">
			<arg line="-s -X PUT"/>
			<arg line="--data-binary @'rdf/datasets.ttl'"/>
			<arg line="-H 'Content-type:text/turtle'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.put.url}?graph=${postprocessingGraphUri}'"/>
		</exec>
		<echo>Uploading resources.ttl to the postprocessing repository...</echo>
		<exec executable="${curl}" searchpath="true" failonerror="true">
			<arg line="-s -X POST"/>
			<arg line="--data-binary @'rdf/resources.ttl'"/>
			<arg line="-H 'Content-type:text/turtle'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.put.url}?graph=${postprocessingGraphUri}'"/>
		</exec>		
	</target>
	
	<target name="postprocessing" depends="upload_temp">
		<echo>[POSTPROCESSING]</echo>
		<echo>Populating the graph URI in the queries...</echo>
		<mkdir dir="sparql/temp/"/>
		<copy todir="sparql/temp/" overwrite="yes">
			<fileset dir="sparql/" includes="postprocessing*"/>
		</copy>
		<replace dir="sparql/temp/" value="${postprocessingGraphUriDecoded}">
			<include name="**/*.*"/>
			<replacetoken>urn:graph:postprocessing</replacetoken>
		</replace>
		<echo>No post-processing for now.</echo>

		<!--<echo>Gathering collected data about resources...</echo>
		<exec executable="${curl}" searchpath="true" failonerror="true" >
			<arg line="-s -X POST"/>
			<arg line="-\-data-binary @'sparql/temp/postprocessing-resources-data.ru'"/>
			<arg line="-H 'Content-type:application/sparql-update'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.sparqlUpdate.url}'"/>
		</exec>-->
		<delete dir="sparql/temp/"/>
	</target>
	
	<target name="metadata" depends="postprocessing">
		<echo>[METADATA]</echo>
		<tstamp>
			<!-- <tstamp> returns the same timestamp all over the script. It's consequently
			better to simply get a date (for now)-->
			<format property="end"
				pattern="yyyy-MM-dd"/>
		</tstamp>
		<mkdir dir="turtle/temp"/>
		<echo>Replacing placeholders in VoID and PROV metadata...</echo>
		<copy todir="turtle/temp">
			<fileset dir="turtle/" includes="*.ttl"/>
		</copy>
		<replace token="@startDate" value="${start}">
			<fileset dir="turtle/temp" includes="*.ttl"/>
		</replace>
		<replace token="@endDate" value="${end}">
			<fileset dir="turtle/temp" includes="*.ttl"/>
		</replace>
		<echo>Uploading metadata to the main graph of the repository...</echo>
		<exec executable="${curl}" searchpath="true">
			<arg line="-s -X PUT"/>
			<arg line="--data-binary @'turtle/temp/prov.ttl'"/>
			<arg line="-H 'Content-type:text/turtle'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.put.url}?default'"/>
		</exec>
		<exec executable="${curl}" searchpath="true">
			<arg line="-s -X POST"/>
			<arg line="--data-binary @'turtle/temp/void.ttl'"/>
			<arg line="-H 'Content-type:text/turtle'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.put.url}?default'"/>
		</exec>
		<delete dir="turtle/temp"/>
	</target>
	
	<target name="publish" depends="metadata">
		<echo>[PUBLISH]</echo>
		<echo>Populating the graph URI in the query...</echo>
		<mkdir dir="sparql/temp/"/>
		<copy todir="sparql/temp/" overwrite="yes">
			<fileset dir="sparql/" includes="replace*"/>
		</copy>
		<replace file="sparql/temp/replace-production-graph.ru" value="${postprocessingGraphUriDecoded}">
			<include name="**/*.*"/>
			<replacefilter token="urn:graph:postprocessing" value="${postprocessingGraphUriDecoded}"/>
			<replacefilter token="urn:graph:production" value="${graphUriDecoded}"/>
		</replace>
		<echo>Replacing old production data with new data...</echo>
		<exec executable="${curl}" searchpath="true" failonerror="true">
			<arg line="-s -X POST"/>
			<arg line="--data-binary @'sparql/temp/replace-production-graph.ru'"/>
			<arg line="-H 'Content-type:application/sparql-update'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.sparqlUpdate.url}'"/>
		</exec>
		<delete dir="sparql/temp/"/>
	</target>
	
	<target name="download-livedata" unless="${livedataDownloaded}">
		<echo>[DOWNLOAD LIVE DATA]</echo>
		<echo>Downloading live data as Trig (Turtle with graphs)...</echo>
		<get
			verbose="false"
			src="${live.repository.get.url}"
			dest="rdf/livedata.trig"/>
	</target>
	
	<target name="livedata" depends="download-livedata">
		<echo>[UPLOAD LIVE DATA]</echo>
		<echo>Uploading live data into the local repository...</echo>
		<exec executable="${curl}" searchpath="true">
			<arg line="-s -X PUT"/>
			<arg line="--data-binary @'rdf/livedata.trig'"/>
			<arg line="-H 'Content-type:text/trig'"/>
			<arg line="-u ${repository.user}:${repository.password}"/>
			<arg line="'${repository.put.url}'"/>
		</exec>
	</target>
</project>

